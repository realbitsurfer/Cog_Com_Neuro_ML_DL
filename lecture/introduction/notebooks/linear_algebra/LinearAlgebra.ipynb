{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b855ba9e",
   "metadata": {},
   "source": [
    "# Linear Algebra\n",
    "\n",
    "We will start with the boring but very important mathematical background that you will need to better understand the machine learning models.\n",
    "\n",
    "Let's start with the general introduction of scalars, vectors and matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc5dbaa",
   "metadata": {},
   "source": [
    "## Scalar, Vectors, Matrices, Tensors\n",
    "\n",
    "The basic objects which we will use throughout this sessions are shown below:\n",
    "\n",
    "![alt text](imgs/svmt_0.png)\n",
    "\n",
    "- **Scalar**: A scalar is just a single number. Variables assigned to scalars are usually lowercase letters. When they are introduced, the space in which they live is usually also indicated, e.g., $a \\in \\mathbb{R}$. This means that $a$ can take any value of the real number space.\n",
    "\n",
    "\n",
    "\n",
    "- **Vectors**: A vector is defined as an array of numbers. In standard Python you would represent a vector as a list of values. Vectors are usually defined with bold lowercase variables, e.g., $\\mathbf{x}$. The order of elments in a vector matters and is defined. The first element of the vector $\\mathbf{x}$ is $x_1$. As this is a scalar, we again use simple lower case variables. The dimensions and number space also has to be defined for a vector. For example $\\mathbf{x} \\in \\mathbb{R}^n$ would indicate that all values in $\\mathbf{x}$ are real and the vector has $n$ elements. <br>  You can think of a vector as identifying a point in a scace. Each element gives the coordinate along a different axis.\n",
    "\n",
    "\n",
    "- **Matrices**: Matrices are 2D-arrays of numbers. We will use bold uppercase variables to define matrices, e.g., $\\mathbf{X}$. If a matrix with only real values has $m$ rows and $n$ colums we would define it as $\\mathbf{X} \\in \\mathbb{R}^{mxn}$. In the above example both, $m$ and $n$ are two. Single elemeents within a matrix are indexed in the order of the dimensions. The first element on the upper left would be $A_{1,1}$, and the last element on the bottom right is  $A_{m,n}$.\n",
    "\n",
    "\n",
    "- **Tensors**: A tensor is a multidimensional object with several axes. The elements of a tensor $\\mathbf{A}$ with three axes would be denoted as $A_{i, j, k}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7981f62",
   "metadata": {},
   "source": [
    "![alt text](imgs/svmt_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a0cc8",
   "metadata": {},
   "source": [
    "## Transpose\n",
    "\n",
    "Transpose is an importantt operation for matrices, as it represents the mirror image of the matrix along a diagonal line called the **main diagonal**. The transpose of a matrix $\\mathbf{X}$ would be denoted as $\\mathbf{X}^T$ and is defined such that\n",
    "$$(\\mathbf{X}^T)_{i,j} = X_j,i $$\n",
    "\n",
    "Vectors can also be thought of as matrices that only contain one column. Hence, when a (column) vector is transposed, the result is a matrix with only one row, or a (row) vector.\n",
    "\n",
    "![alt text](imgs/transpose.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e487c6",
   "metadata": {},
   "source": [
    "# Mathematical operations with matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f2219b",
   "metadata": {},
   "source": [
    "## Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f5809",
   "metadata": {},
   "source": [
    "A scalar can be easily added to a matrix, just by adding the scalar to each element of the matrix: $a + X = C$, where $C_{i, j} = a + X_{i, j}$.\n",
    "\n",
    "\n",
    "![alt text](imgs/addition_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0469e7da",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Matrices can be easily added, as long as they have the same shape. Therefore, their corresponding elements are added: $\\mathbf{X} + \\mathbf{A} = \\mathbf{C}$, where $C_{i, j} = X_{i, j} + A_{i, j}$.\n",
    "\n",
    "![alt text](imgs/addition_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d6d200",
   "metadata": {},
   "source": [
    "In deep learning it is also possible to add a vector to a matrix. In this case the vector is simply repeated so that it can be added to each row of the matrix: $\\mathbf{C} = \\mathbf{X} + \\mathbf{a}$, where $C_{i, j} = X_{i, j} + b_j$.  This concept is called **broadcasting**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ae647f",
   "metadata": {},
   "source": [
    "## Multiplication\n",
    "\n",
    "### Dot Product\n",
    "\n",
    "Two vectors can be multiplied using the so called dot product between those two. This is a bit more advanced than the addition, as not each element of the vectors are multiplied with each other. It is denoted as:\n",
    "\n",
    "$$x^Ty = \\sum_i x_iy_i$$\n",
    "\n",
    "Hence, the vectors are multiplied element-wise and in the end all results are summed. This also implies that the dot product can only be calculated if the vectors have the same length. \n",
    "\n",
    "![alt text](imgs/multi_0.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9922c6",
   "metadata": {},
   "source": [
    "Vector multiplications are `distributive`:\n",
    "\n",
    "$$ x^T (y + z) = x^Ty + x^Tz $$\n",
    "\n",
    "\n",
    "And they are `commutative`:\n",
    "\n",
    "$$ x^T y = y^Tx $$\n",
    "\n",
    "But the dot product is not `assocciative`, as $(x^Ty)z$ is the dot product between a scalar and a vector, which is not defined. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc89764",
   "metadata": {},
   "source": [
    "### Matrix Multiplication\n",
    "\n",
    "The matrix mulitplication is an operation that is performed regularly in machine or deep learning. The **matrix product** of two matrices ($\\mathbf{X}$ and $\\mathbf{A}$) is again a matrix ($\\mathbf{C}$). \n",
    "\n",
    "$$ \\mathbf{C} = \\mathbf{X} \\mathbf{A}$$\n",
    "\n",
    "A matrix multiplication is only allowed if the shapes of the matrices to be multiplied matches. In this case, the matrix $\\mathbf{X}$ needs to have the same amount of columns as the number of rows in matrix $\\mathbf{A}$. If $\\mathbf{X}$ has the shape $n x m$ and $\\mathbf{A}$ is of shape $m x p$, then the dimensions of the resulting matrix $\\mathbf{C}$ are $n x p$. \n",
    "\n",
    "For each element in $\\mathbf{C}$, the matrix product can be written as:\n",
    "\n",
    "$$ C_{i, j} = \\sum_k X_{i, k}A_{k, j}$$\n",
    "\n",
    "Visually this can be represented as follows\n",
    "\n",
    "![alt text](imgs/multi_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6132535",
   "metadata": {},
   "source": [
    "Matrix multiplications are `distributive`:\n",
    "\n",
    "$$ \\mathbf{A} (\\mathbf{B} + \\mathbf{C}) = \\mathbf{A}\\mathbf{B} + \\mathbf{A}\\mathbf{C}$$\n",
    "\n",
    "They are also `assocciative`:\n",
    "\n",
    "$$ \\mathbf{A} (\\mathbf{B}\\mathbf{C}) = (\\mathbf{A}\\mathbf{B})\\mathbf{C} $$\n",
    "\n",
    "However, it is **not** `commutative`:\n",
    "\n",
    "$$ \\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A} $$\n",
    "\n",
    "Note, that some special cases are commutative, however not generally.\n",
    "\n",
    "The transpose of a matrix product can be represented as:\n",
    "$$ (\\mathbf{A}\\mathbf{B})^T = \\mathbf{B}^T\\mathbf{A}^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd864b1d",
   "metadata": {},
   "source": [
    "# Linear equations\n",
    "\n",
    "Now that we know how to do matrix multiplication, we can use this to write down a simple linear equatiton. Assume we have $N$ observations of variable x that has $m$ features. Take as an example different features of appartments such as flat size, year it was built, and so on. We can store this information in a matrix $X$ that has the dimensions ($N$, $m$) i.e., $X \\in \\mathbb{R}^{Nxm}$. If we now want to determine the prize for a given flat, we could use different scaling factors for each feature. Hence in an additional vector $w$ we store how much each feature affects the prize. To now calculate the prize $p$ we can write: \n",
    "$$ \\mathbf{Xw} = \\mathbf{p} $$\n",
    "\n",
    "More explicitly this would mean:\n",
    "\n",
    "$$ X_{1, 1}w_1 + X_{1, 2}w_2 + ... + X_{1, m}w_m = b_1$$\n",
    "$$ X_{2, 1}w_1 + X_{2, 2}w_2 + ... + X_{2, m}w_m = b_2$$\n",
    "\n",
    "We can also rewrite this as:\n",
    "\n",
    "$$ \\mathbf{Xw}  = \\sum_i w_i \\mathbf{X_{:,i}} = \\mathbf{p}$$\n",
    "\n",
    "where $\\mathbf{X_{:,i}}$ denotes that we perform this operation across all rows while varying the column with index $i$. This is also called a $linear combination$.\n",
    "\n",
    "This representation is especially important for deep learning including any form of linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969daeb7",
   "metadata": {},
   "source": [
    "## Linear dependence\n",
    "\n",
    "A vector is linearly dependent on a set of vectors, if the vector can be represented with a linear combination of this set of vectors. \n",
    "On the other hand, a set of vectors are linearly independent if no vector is a linear combination of any other vectors. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d56b5d8",
   "metadata": {},
   "source": [
    "# Matrix Inversion\n",
    "\n",
    "It is not possible to directly divide by a matrix. However, we can utilize the concept of matrix inversion to be able to solve a linear system of equations as the one above for $w$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775fbe32",
   "metadata": {},
   "source": [
    "## Identitty matrix\n",
    "\n",
    "For matrix inversioon we first need to define the concept of the identity matrix. The identitty matrix is denoted as $\\mathbf{I_n} \\in \\mathbb{R}^{nxn}$. This special matrix contains only ones on its diagonal. All other entries are zero. \n",
    "\n",
    "<img src=\"imgs/identity.png\" alt=\"Identity\" style=\"width: 100px;\"/>\n",
    "\n",
    "Multiplying any vector with this matrix does not change the vector. \n",
    "$$\\forall x \\in \\mathbb{R} ^n, \\mathbf{I_n x} = \\mathbf{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82dddd1e",
   "metadata": {},
   "source": [
    "## Matrix inverse\n",
    "\n",
    "Now we can define the matrix inverse of a given matrix $X$ as $X^{-1}$ such that\n",
    "\n",
    "$$\\mathbf{X^{-1}X} = \\mathbf{I_n}$$\n",
    "\n",
    "\n",
    "However, for $\\mathbf{X^{-1}}$ to exist $\\mathbf{X}$ has to be a square matrix (number of rows = number of columns), all columns must be linearly independent, hence the determinant, to which we will come later, has to be non-zero. \n",
    "\n",
    "Defining the matrix inverse now also allows us to solve the above defined linear system of equations for $w$. \n",
    "\n",
    "$$\\mathbf{Xw} = \\mathbf{b} $$\n",
    "$$\\mathbf{X^{-1}Xw} = \\mathbf{X^{-1}b} $$\n",
    "$$\\mathbf{I_nw} = \\mathbf{X^{-1}b} $$\n",
    "$$\\mathbf{w} = \\mathbf{X^{-1}b} $$\n",
    "\n",
    "Note, that calculating the inverse can often times be computatioonally inefficient and contain limited precision on a computer. Hence this representation is usually only for theoretical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb2398c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Norms, Trace?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2863ee1",
   "metadata": {},
   "source": [
    "# Norms\n",
    "\n",
    "We can measure the size (or length) of a vector with the help of the `norm`. It measurs the distance from the origin to the point defined by the vector. \n",
    "\n",
    "Generally the $L^p$ norm is defined as\n",
    "\n",
    "$$ ||\\mathbf{x}||_p = \\left(\\sum_i |x_i|^p\\right)^{\\frac{1}{p}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be1836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Special matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af7445a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Eigendecomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6f0c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Determinant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a134cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d06bd2f4",
   "metadata": {},
   "source": [
    "## Numpy exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f787760f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
